---
---

@misc{li2025visionlanguagemodelsmap,
      title={Vision Language Models Map Logos to Text via Semantic Entanglement in the Visual Projector}, 
      author={Sifan Li and Hongkai Chen and Yujun Cai and Qingwen Ye and Liyang Chen and Junsong Yuan and Yiwei Wang},
      year={2025},
      eprint={2510.12287},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2510.12287}, 
      arxiv={2510.12287},
      pdf={logotext.pdf},
      preview={logos.png},
      selected={true},
      website={https://github.com/johnnyZeppelin/LogoText}
}

@misc{li2025semvinkadvancingvlmssemantic,
      title={SemVink: Advancing VLMs' Semantic Understanding of Optical Illusions via Visual Global Thinking}, 
      author={Sifan Li and Yujun Cai and Yiwei Wang},
      year={2025},
      eprint={2506.02803},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2506.02803},
      note={Main Conference of EMNLP 2025.},
      arxiv={2506.02803},
      pdf={semvink_emnlp25.pdf},
      poster={semvink_poster.pdf},
      slides={semvink_slides.pdf},
      preview={semvink.png},
      selected={true},
      website={https://johnnyzeppelin.github.io/vlm-semvink/}
}

@misc{li2025newsnowtabletscontain,
      title={Do "New Snow Tablets" Contain Snow? Large Language Models Over-Rely on Names to Identify Ingredients of Chinese Drugs}, 
      author={Sifan Li and Yujun Cai and Bryan Hooi and Nanyun Peng and Yiwei Wang},
      year={2025},
      eprint={2504.03786},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2504.03786},
      arxiv={2504.03786},
      pdf={2504.03786v3.pdf},
      poster={bezoar.pdf},
      preview={cpm.png},
      selected={true},
      website={https://med-llm.github.io/tcm-llm-overrely-on-names/}
}

@misc{li2025replacetranslationboostconcept,
      title={Replace in Translation: Boost Concept Alignment in Counterfactual Text-to-Image}, 
      author={Sifan Li and Ming Tao and Hao Zhao and Ling Shao and Hao Tang},
      year={2025},
      eprint={2505.14341},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2505.14341},
      arxiv={2505.14341},
      pdf={2505.14341v1.pdf},
      poster={replace.pdf},
      selected={true},
      preview={replace.png},
}


@ARTICLE{bpgi,
  author={Liu, Yun and Li, Sifan and Liu, Zihan and Wang, Haiyuan and Fan, Daoxin},
  journal={IEEE Open Journal on Immersive Displays (accepted)}, 
  title={BPGI: A Brain-Perception Guided Interactive Network for Stereoscopic Omnidirectional Image Quality Assessment}, 
  year={2025},
  volume={},
  number={}
}

@article{tmm2025,
  title = {TFFN: Three-Branch Feature Fusion Network for Stereoscopic Omnidirectional Image Quality Assessment},
  journal = {IEEE Transactions on Multimedia (accepted)},
  
  author = {Liu, Yun and Li, Sifan
            and Fan, Daoxin
            and Duan, Huiyu
            and Jing, Peiguang},
year={2025},
  volume={},
  number={}
}

@ARTICLE{10929024,
  author={Liu, Yun and Li, Sifan and Duan, Huiyu and Zhou, Yu and Fan, Daoxin and Zhai, Guangtao},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Multi-Task Guided No-Reference Omnidirectional Image Quality Assessment With Feature Interaction}, 
  year={2025},
  volume={35},
  number={9},
  pages={8794-8806},
  keywords={Feature extraction;Visualization;Image quality;Measurement;Degradation;Solid modeling;Data mining;Multitasking;Distortion;Adaptation models;Bidirectional pseudo reference;omnidirectional image quality assessment;Mamba;multi-scale aggregation;multi-task learning;no-reference (NR)},
  doi={10.1109/TCSVT.2025.3551723}
}


@InProceedings{Li:10.1007/978-981-97-3626-3_4,
author="Liu, Yun
and Wen, Zhipeng
and Li, Sifan
and Fan, Daoxin
and Zhai, Guangtao",
editor="Zhai, Guangtao
and Zhou, Jun
and Ye, Long
and Yang, Hua
and An, Ping
and Yang, Xiaokang",
title="Image Aesthetics Assessment Based on Visual Perception and Textual Semantic Understanding",
booktitle="Digital Multimedia Communications",
year="2024",
publisher="Springer Nature Singapore",
address="Singapore",
pages="39--53",
abstract="Our goal is to promote an effective image aesthetics assessment (IAA) model. In the current Internet era, it has become easier to obtain the text description of an image. With the dual-modal support of image and text, the image aesthetics assessment model will further reflect its superiority. To this end, we design a multimodal feature-driven guided image aesthetics assessment model (MFD). Firstly, multi-modal features are extracted through the feature extraction sub-network, including image-driven aesthetic features and content features, as well as text-driven semantic features. Each feature captures the implicit characteristics of different levels of human brain object analysis. Secondly, these multi-modal features are combined to form multi-modal combination features that contain multiple characteristics. Finally, the obtained multi-modal are combined for aesthetic assessment prediction. Experimental results on public image aesthetics assessment databases demonstrate the superiority of our model.",
isbn="978-981-97-3626-3",
doi={https://doi.org/10.1007/978-981-97-3626-3_4}
}

@InProceedings{10.1007/978-981-97-5603-2_11,
author="Liu, Yun
and Wen, Zhipeng
and Jin, Minzhu
and Fan, Daoxin
and Li, Sifan
and Liu, Bo
and Jiang, Jinhe
and Xiao, Xianda",
editor="Huang, De-Shuang
and Chen, Wei
and Pan, Yijie",
title="A Multimodal Fake News Detection Model with Self-supervised Unimodal Label Generation",
booktitle="Advanced Intelligent Computing Technology and Applications",
year="2024",
publisher="Springer Nature Singapore",
address="Singapore",
pages="130--141",
abstract="Fake news detection has become a hot topic. Most multimodal fake news detection models only focus on the semantic correlation between single modalities and often ignore the semantic differences between single modalities, which limited the performance. To deal with the above problem, this paper proposes a multimodal fake news detection model (AFUG), which fully pays attention to the semantic correlation between each modal information by designing a cross-modal fusion module. The self-supervised unimodal label generation model is also added to constrain the overall model optimization. In order to focus on samples with highly differentiated modal information, we design an adaptive weight adjustment strategy to guide the model's learning of unimodal information. Extensive experiments on two datasets demonstrate the effectiveness of our AFUG.",
isbn="978-981-97-5603-2",
doi={https://doi.org/10.1007/978-981-97-5603-2_11}
}
